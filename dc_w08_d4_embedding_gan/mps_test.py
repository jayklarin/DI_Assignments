# =========================================================
# mps_test.py ‚Äî Quick MPS + BERT sanity check
# =========================================================
import torch
from transformers import BertTokenizer, BertModel

print("üî• Starting MPS sanity check...")

# ---------------------------------------------------------
# 1Ô∏è‚É£  Select device (MPS if available)
# ---------------------------------------------------------
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("‚úÖ MPS is available. Using Apple GPU.")
else:
    device = torch.device("cpu")
    print("‚ö†Ô∏è MPS not available. Falling back to CPU.")

# ---------------------------------------------------------
# 2Ô∏è‚É£  Load model + tokenizer
# ---------------------------------------------------------
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased").to(device)
model.eval()

# ---------------------------------------------------------
# 3Ô∏è‚É£  Create sample text and tokenize
# ---------------------------------------------------------
sample_text = ["This is a quick test to verify MPS execution."]
inputs = tokenizer(sample_text, return_tensors="pt", padding=True, truncation=True)
inputs = {k: v.to(device) for k, v in inputs.items()}

# ---------------------------------------------------------
# 4Ô∏è‚É£  Run forward pass
# ---------------------------------------------------------
with torch.no_grad():
    outputs = model(**inputs)

print("‚úÖ Forward pass successful.")
print("Hidden state shape:", outputs.last_hidden_state.shape)
print("Pooled output shape:", outputs.pooler_output.shape)

# ---------------------------------------------------------
# 5Ô∏è‚É£  Confirm GPU memory use
# ---------------------------------------------------------
if device.type == "mps":
    print("GPU tensors allocated:", torch.mps.current_allocated_memory() / 1e6, "MB")

print("üéØ MPS test complete.")
