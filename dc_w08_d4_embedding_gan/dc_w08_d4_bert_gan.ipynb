{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b5b4fd",
   "metadata": {},
   "source": [
    "## üì¶ Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec2f6c0",
   "metadata": {},
   "source": [
    "Imports all required libraries for data handling, PyTorch model building, and Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d230a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertEncoder\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f09ba4",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Device Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade2646",
   "metadata": {},
   "source": [
    "Selects the fastest available device (MPS ‚Üí Metal GPU on Mac, otherwise CPU) and clears any cached GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39588ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Device setup ‚Äî works on CUDA (Colab), MPS (Apple), or CPU\n",
    "# =========================================================\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"‚úÖ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"‚úÖ Using Apple GPU (MPS)\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚öôÔ∏è  Using CPU (no GPU detected)\")\n",
    "\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2717b1",
   "metadata": {},
   "source": [
    "## üìÅ File Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab95b43",
   "metadata": {},
   "source": [
    "Defines where to load and save model/data artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"./data/train_essays.csv\"\n",
    "TEST_PATH = \"./data/test_essays.csv\"\n",
    "PROMPT_PATH = \"./data/train_prompts.csv\"\n",
    "tokenizer_save_path = \"./models/tokenizer\"\n",
    "model_save_path = \"./models/bert-base\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4957e6d",
   "metadata": {},
   "source": [
    "## üìÇ Load and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ace42",
   "metadata": {},
   "source": [
    "Reads CSV files, renames columns for clarity, and confirms dataset shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a91652",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = pd.read_csv(TRAIN_PATH)\n",
    "src_test = pd.read_csv(TEST_PATH)\n",
    "src_prompt = pd.read_csv(PROMPT_PATH)\n",
    "print(\"‚úÖ Files loaded:\")\n",
    "print(f\"Train: {src_train.shape}, Test: {src_test.shape}, Prompts: {src_prompt.shape}\")\n",
    "src_train.rename(columns={\"text\": \"essay_text\", \"generated\": \"label\"}, inplace=True)\n",
    "src_test.rename(columns={\"text\": \"essay_text\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef8bf4",
   "metadata": {},
   "source": [
    "## ü§ó Tokenizer and Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86bd9f4",
   "metadata": {},
   "source": [
    "Loads BERT base uncased, moves it to MPS/CPU, and saves tokenizer + model locally for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d7579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "pretrained_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "embedding_model = pretrained_model.bert.to(device)\n",
    "os.makedirs(tokenizer_save_path, exist_ok=True)\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "pretrained_model.save_pretrained(model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77995c",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Training Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074093d1",
   "metadata": {},
   "source": [
    "Defines batch sizes, learning rate, and GAN/BERT architecture settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1394bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "test_batch_size = 16\n",
    "lr = 1e-4\n",
    "beta1 = 0.5\n",
    "nz = 100\n",
    "num_epochs = 3\n",
    "num_hidden_layers = 6\n",
    "train_ratio = 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e6625d",
   "metadata": {},
   "source": [
    "## üß© Dataset Class and Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c1d692",
   "metadata": {},
   "source": [
    "Creates a custom PyTorch Dataset and splits the training set into train/test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ecbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDAIGDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        return (text, self.labels[idx]) if self.labels is not None else text\n",
    "\n",
    "all_num = len(src_train)\n",
    "train_num = int(all_num * train_ratio)\n",
    "train_set = src_train.sample(train_num, random_state=42)\n",
    "test_set = src_train.drop(train_set.index).reset_index(drop=True)\n",
    "train_dataset = GANDAIGDataset(train_set[\"essay_text\"].tolist(), train_set[\"label\"].tolist())\n",
    "test_dataset  = GANDAIGDataset(test_set[\"essay_text\"].tolist(),  test_set[\"label\"].tolist())\n",
    "train_loader  = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader   = DataLoader(test_dataset,  batch_size=test_batch_size,  shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd2fa8a",
   "metadata": {},
   "source": [
    "## üß† Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e93886",
   "metadata": {},
   "source": [
    "Defines the Generator, Discriminator, and pooling layer for GAN-BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f4048",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(num_hidden_layers=num_hidden_layers)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, 256 * 128)\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(128, 768, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.bert_encoder = BertEncoder(config)\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x).view(-1, 128, 256).permute(0, 2, 1)\n",
    "        x = self.conv_net(x).permute(0, 2, 1)\n",
    "        return self.bert_encoder(x)\n",
    "\n",
    "class SumBertPooler(nn.Module):\n",
    "    def forward(self, hidden_states):\n",
    "        sum_hidden = hidden_states.sum(dim=1)\n",
    "        denom = torch.clamp(sum_hidden.sum(1).unsqueeze(1), min=1e-9)\n",
    "        return sum_hidden / denom\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert_encoder = BertEncoder(config)\n",
    "        self.bert_encoder.layer = nn.ModuleList(\n",
    "            [layer for layer in pretrained_model.bert.encoder.layer[:6]]\n",
    "        )\n",
    "        self.pooler = SumBertPooler()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 256), nn.ReLU(), nn.Linear(256, 1)\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        out = self.bert_encoder(input)\n",
    "        out = self.pooler(out.last_hidden_state)\n",
    "        out = self.classifier(out)\n",
    "        return torch.sigmoid(out).view(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed9c0b",
   "metadata": {},
   "source": [
    "## üßÆ Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c870d86",
   "metadata": {},
   "source": [
    "Evaluates AUC score and creates text ‚Üí embedding conversion utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb94c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_auc(model):\n",
    "    model.eval(); preds, acts = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            enc = tokenizer(batch[0], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            embed = embedding_model(**enc).last_hidden_state\n",
    "            lab = batch[1].float().to(device)\n",
    "            out = model(embed)\n",
    "            preds.extend(out.cpu().numpy()); acts.extend(lab.cpu().numpy())\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    try: auc = roc_auc_score(acts, preds)\n",
    "    except ValueError: auc = 0.5\n",
    "    print(\"AUC:\", auc); return auc\n",
    "\n",
    "def preparation_embedding(texts):\n",
    "    enc = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    return embedding_model(**enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969300b7",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÇÔ∏è Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883eaa12",
   "metadata": {},
   "source": [
    "Initializes networks, optimizers, and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46302098",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(input_dim=nz).to(device)\n",
    "netD = Discriminator().to(device)\n",
    "criterion  = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ffbbc",
   "metadata": {},
   "source": [
    "## üîÅ Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91296b50",
   "metadata": {},
   "source": [
    "Performs alternating GAN updates and prints losses every 20 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_step(optimizerG, optimizerD, netG, netD, real_data, label, epoch, i):\n",
    "    netD.zero_grad()\n",
    "    out = netD(real_data)\n",
    "    errD_real = criterion(out, label); errD_real.backward()\n",
    "    D_x = out.mean().item()\n",
    "    noise = torch.randn(real_data.size(0), nz, device=device)\n",
    "    fake_data = netG(noise).last_hidden_state\n",
    "    label.fill_(1); out = netD(fake_data.detach())\n",
    "    errD_fake = criterion(out, label); errD_fake.backward()\n",
    "    D_G_z1 = out.mean().item()\n",
    "    errD = errD_real + errD_fake; optimizerD.step()\n",
    "    netG.zero_grad(); label.fill_(0)\n",
    "    out = netD(fake_data); errG = criterion(out, label)\n",
    "    errG.backward(); D_G_z2 = out.mean().item(); optimizerG.step()\n",
    "    if i % 20 == 0:\n",
    "        print(f\"[{epoch}/{num_epochs}][{i}/{len(train_loader)}] \"\n",
    "              f\"Loss_D:{errD.item():.4f} Loss_G:{errG.item():.4f} \"\n",
    "              f\"D(x):{D_x:.4f} D(G(z)):{D_G_z1:.4f}/{D_G_z2:.4f}\")\n",
    "    return optimizerG, optimizerD, netG, netD\n",
    "\n",
    "model_infos=[]\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        with torch.no_grad(): embed = preparation_embedding(data[0])\n",
    "        optimizerG, optimizerD, netG, netD = GAN_step(\n",
    "            optimizerG, optimizerD, netG, netD,\n",
    "            real_data=embed.last_hidden_state.to(device),\n",
    "            label=data[1].float().to(device),\n",
    "            epoch=epoch, i=i)\n",
    "    auc_score = eval_auc(netD)\n",
    "    model_infos.append({\"epoch\": epoch, \"auc_score\": auc_score})\n",
    "print(\"‚úÖ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a12e30",
   "metadata": {},
   "source": [
    "## üßæ Inference and Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54380ef0",
   "metadata": {},
   "source": [
    "Runs the trained discriminator on test essays and saves predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = GANDAIGDataset(src_test[\"essay_text\"].tolist())\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "netD.eval(); preds=[]\n",
    "with torch.no_grad():\n",
    "    for batch in inference_loader:\n",
    "        enc = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        embed = embedding_model(**enc).last_hidden_state\n",
    "        out = netD(embed); preds.extend(out.cpu().numpy())\n",
    "sub_df = pd.DataFrame({\"id\": src_test[\"id\"], \"prediction\": preds})\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "sub_df.to_csv(\"outputs/submission.csv\", index=False)\n",
    "print(\"‚úÖ Inference complete! Saved to outputs/submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
